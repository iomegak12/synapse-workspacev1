{
	"name": "Notebook 5",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkPool2v20",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "67f7e2e1-3fe0-497b-9997-94b7108937f5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/924cf5c7-5f72-42ba-98ee-7ea905016982/resourceGroups/trainingresourcegroupv2/providers/Microsoft.Synapse/workspaces/synapseworkspacev10/bigDataPools/SparkPool2v20",
				"name": "SparkPool2v20",
				"type": "Spark",
				"endpoint": "https://synapseworkspacev10.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkPool2v20",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"create database CosmosDBIoTDemo"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"create table if not exists CosmosDBIoTDemo.IoTSignals\r\n",
					"using cosmos.olap\r\n",
					"options(spark.synapse.linkedService 'CosmosDBIoTDemo',\r\n",
					"        spark.cosmos.container 'IoTSignals')"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"create table if not exists CosmosDBIoTDemo.IoTDeviceInfo\r\n",
					"using cosmos.olap\r\n",
					"options(spark.synapse.linkedService 'CosmosDBIoTDemo',\r\n",
					"        spark.cosmos.container 'IoTDeviceInfo')"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_RPM_details = spark.sql(\"select a.deviceid \\\r\n",
					"                                 , b.devicetype \\\r\n",
					"                                 , cast(b.location as string) as location\\\r\n",
					"                                 , cast(b.latitude as float) as latitude\\\r\n",
					"                                 , cast(b.longitude as float) as  longitude\\\r\n",
					"                                 , a.measuretype \\\r\n",
					"                                 , a.unitSymbol \\\r\n",
					"                                 , cast(sum(measureValue) as float) as measureValueSum \\\r\n",
					"                                 , count(*) as count \\\r\n",
					"                            from CosmosDBIoTDemo.IoTSignals a \\\r\n",
					"                            left join CosmosDBIoTDemo.IoTDeviceInfo b \\\r\n",
					"                            on a.deviceid = b.deviceid \\\r\n",
					"                            where a.unitSymbol = 'RPM' \\\r\n",
					"                            group by a.deviceid, b.devicetype, b.location, b.latitude, b.longitude, a.measuretype, a.unitSymbol\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_RPM_details)\r\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from plotly.offline import plot\r\n",
					"import plotly.express as px\r\n",
					"\r\n",
					"df_RPM_details_pd = df_RPM_details.toPandas()\r\n",
					"fig = px.scatter_mapbox(df_RPM_details_pd, \r\n",
					"                        lat='latitude', \r\n",
					"                        lon='longitude', \r\n",
					"                        size = 'measureValueSum',\r\n",
					"                        color = 'measureValueSum',\r\n",
					"                        hover_name = 'location',\r\n",
					"                        hover_data = ['measureValueSum','location'],\r\n",
					"                        size_max = 30,\r\n",
					"                        color_continuous_scale = px.colors.carto.Temps,\r\n",
					"                        zoom=3,\r\n",
					"                        height=600,\r\n",
					"                        width =900)\r\n",
					"\r\n",
					"fig.update_layout(mapbox_style='open-street-map')\r\n",
					"fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
					"\r\n",
					"p = plot(fig,output_type='div')\r\n",
					"displayHTML(p)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_IoTSignals = spark.read\\\r\n",
					"                    .format(\"cosmos.olap\")\\\r\n",
					"                    .option(\"spark.synapse.linkedService\", \"CosmosDBIoTDemo\")\\\r\n",
					"                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\r\n",
					"                    .load()"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"\r\n",
					"df_IoTSignals_pd = df_IoTSignals.toPandas().dropna()\r\n",
					"df_IoTSignals_pd['measureValue'] = df_IoTSignals_pd['measureValue'].astype(int)\r\n",
					"\r\n",
					"df_MW = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'MW')]\r\n",
					"df_MW.plot(x='dateTime', y='measureValue', color='green', figsize=(20,5), label = 'Output MW')\r\n",
					"plt.title('MW TimeSeries')\r\n",
					"plt.show()\r\n",
					"\r\n",
					"df_RPM = df_IoTSignals_pd[(df_IoTSignals_pd['deviceId'] == 'dev-1') & (df_IoTSignals_pd['unitSymbol'] == 'RPM')]\r\n",
					"df_RPM.plot(x='dateTime', y='measureValue', color='black', figsize=(20,5), label = 'Output RPM')\r\n",
					"plt.title('RPM TimeSeries')\r\n",
					"plt.show()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import *\r\n",
					"from mmlspark.cognitive import SimpleDetectAnomalies\r\n",
					"from mmlspark.core.spark import FluentAPI\r\n",
					"\r\n",
					"anomaly_detector = (SimpleDetectAnomalies()\r\n",
					"                            .setSubscriptionKey(\"fab18b6f2fbd4d2996fe84490f2c171c\")\r\n",
					"                            .setUrl(\"https://traininganomalydetectorv10.cognitiveservices.azure.com/anomalydetector/v1.0/timeseries/entire/detect\")\r\n",
					"                            .setOutputCol(\"anomalies\")\r\n",
					"                            .setGroupbyCol(\"grouping\")\r\n",
					"                            .setSensitivity(95)\r\n",
					"                            .setGranularity(\"secondly\"))\r\n",
					"\r\n",
					"df_anomaly = (df_IoTSignals\r\n",
					"                    .where(col(\"unitSymbol\") == 'RPM')\r\n",
					"                    .withColumnRenamed(\"dateTime\", \"timestamp\")\r\n",
					"                    .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\r\n",
					"                    .withColumn(\"grouping\", col(\"deviceId\"))\r\n",
					"                    .mlTransform(anomaly_detector))\r\n",
					"\r\n",
					"df_anomaly.createOrReplaceTempView('df_anomaly')"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df_anomaly_single_device = spark.sql(\"select timestamp \\\r\n",
					"                                            , measureValue \\\r\n",
					"                                            , anomalies.expectedValue \\\r\n",
					"                                            , anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue \\\r\n",
					"                                            , anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue \\\r\n",
					"                                            , case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly \\\r\n",
					"                                        from df_anomaly \\\r\n",
					"                                        where deviceid = 'dev-1' \\\r\n",
					"                                        order by timestamp \\\r\n",
					"                                        limit 400\")\r\n",
					"\r\n",
					"display(df_anomaly_single_device)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import plotly as py\r\n",
					"import plotly.graph_objs as go\r\n",
					"from plotly.offline import plot\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from matplotlib.pyplot import figure\r\n",
					" \r\n",
					"adf = df_anomaly_single_device.toPandas()\r\n",
					"adf_subset = df_anomaly_single_device.where(col(\"isAnomaly\") == 1).toPandas() \r\n",
					"\r\n",
					"plt.figure(figsize=(23,8))\r\n",
					"plt.plot(adf['timestamp'],adf['expectedUpperValue'], color='darkred', linestyle='solid', linewidth=0.25)\r\n",
					"plt.plot(adf['timestamp'],adf['expectedValue'], color='darkgreen', linestyle='solid', linewidth=2)\r\n",
					"plt.plot(adf['timestamp'],adf['measureValue'], 'b', color='royalblue', linestyle='dotted', linewidth=2)\r\n",
					"plt.plot(adf['timestamp'],adf['expectedLowerValue'],  color='black', linestyle='solid', linewidth=0.25)\r\n",
					"plt.plot(adf_subset['timestamp'],adf_subset['measureValue'], 'ro')\r\n",
					"plt.legend(['RPM-UpperMargin', 'RPM-ExpectedValue', 'RPM-ActualValue', 'RPM-LowerMargin', 'RPM-Anomaly'])\r\n",
					"plt.title('RPM Anomalies with Expected, Actual, Upper and Lower Values')\r\n",
					"plt.show()"
				],
				"execution_count": 13
			}
		]
	}
}